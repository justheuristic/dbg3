python3 train_lm.py \
  --initial_peers "$INIT_PEER" \
  --dataset_path "$INPUT_PATH"/data/albert_tokenized_bookcorpus_wiki/train \
  --cache_dir "$INPUT_PATH" \
  --config_path "$INPUT_PATH"/albert_config_xxlarge.json \
  --tokenizer_path "$INPUT_PATH"/tokenizer \
  --do_train --per_device_train_batch_size 1 \
  --dataloader_num_workers 4 \
  --gradient_accumulation_steps 4 \
  --learning_rate 0.00176 --max_grad_norm 1.0 \
  --adam_epsilon 1e-6 --weight_decay 0.01 \
  --output_dir "$SNAPSHOT_PATH" --overwrite_output_dir \
  --save_steps 500 --save_total_limit 2 \
  --max_steps 125000 --warmup_steps 5000 \
  --logging_steps 100 --grid-size 4
